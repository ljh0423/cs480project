{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DED6s0jfKq9s"
      },
      "source": [
        "#Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "edit the data directory in block 4\n",
        "\n",
        "run code blocks in order to train model2\n",
        "(may cause out of memory error especially when run with model 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1ui5hjlJkn5"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import imageio.v3 as imageio\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC-zWFZVKnxL"
      },
      "outputs": [],
      "source": [
        "n_epochs = 6\n",
        "n_batch_size = 10\n",
        "n_display_step = 10\n",
        "n_learning_rate = 0.0001\n",
        "\n",
        "output_col = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']\n",
        "output_col_test = ['X4', 'X11', 'X18', 'X26', 'X50', 'X3112']\n",
        "\n",
        "datapath = '/kaggle/input/cs-480-2024-spring/data/' # edit to directory with data\n",
        "train = pd.read_csv(datapath + 'train.csv')\n",
        "train['path'] = [datapath + 'train_images/'+str(int(e))+'.jpeg' for e in train['id'].values]\n",
        "train['jb'] = train['path'].apply(lambda fp: open(fp, 'rb').read())\n",
        "\n",
        "test = pd.read_csv(datapath + 'test.csv')\n",
        "test['path'] = [datapath + 'test_images/'+str(int(e))+'.jpeg' for e in test['id'].values]\n",
        "test['jb'] = test['path'].apply(lambda fp: open(fp, 'rb').read())\n",
        "\n",
        "\n",
        "# remove outliers\n",
        "for column in output_col:\n",
        "    lower_quantile = train[column].quantile(0.005)\n",
        "    upper_quantile = train[column].quantile(0.985)\n",
        "    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n",
        "\n",
        "print(train.shape)\n",
        "\n",
        "log_feat = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']\n",
        "\n",
        "y_train_log = np.zeros_like(train[output_col])\n",
        "\n",
        "for i, tar in enumerate(output_col):\n",
        "    col = train[tar].values\n",
        "    if tar in log_feat:\n",
        "        col = np.log10(col)\n",
        "    y_train_log[:, i] = col\n",
        "\n",
        "\n",
        "SCALER = StandardScaler()\n",
        "y_train_norm = SCALER.fit_transform(y_train_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWxU-oF5BruM"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, data_im, data_out, transform=None, mode='m1train'):\n",
        "        self.data_im = data_im\n",
        "        self.data_out = data_out\n",
        "        self.transform = transform\n",
        "        self.mode = mode\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.transform(imageio.imread(self.data_im[index]))\n",
        "        y = self.data_out[index]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_im)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wfxz1Aca2nF"
      },
      "outputs": [],
      "source": [
        "test_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((384, 384)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter((0.9,1.1), (0.9,1.1)),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "trainx, valx,trainy, valy = train_test_split(train, y_train_norm, test_size=2000)\n",
        "\n",
        "train_dataset = MyDataset(trainx['jb'].values, trainy, train_transform, 'm2train')\n",
        "val_dataset = MyDataset(valx['jb'].values, valy, test_transform, 'm2train')\n",
        "test_dataset = MyDataset(test['jb'].values, test['id'].values, test_transform, 'm2test')\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=n_batch_size, drop_last=True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=n_batch_size, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWUp4t1ibFA3"
      },
      "outputs": [],
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.swin = timm.create_model(\n",
        "                'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "                num_classes=6,\n",
        "                pretrained=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.swin(input)\n",
        "\n",
        "model2 = Model2()\n",
        "model2 = model2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuKdgZslB5lN"
      },
      "outputs": [],
      "source": [
        "def lr_sc(optimizer):\n",
        "    return torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer=optimizer,\n",
        "        max_lr=1e-4,\n",
        "        total_steps=len(train_dataset)//n_batch_size * n_epochs + 1,\n",
        "        pct_start=0.1,\n",
        "        anneal_strategy='cos',\n",
        "        div_factor=1e1,\n",
        "        final_div_factor=1e1,\n",
        "    )\n",
        "\n",
        "optimizer = optim.AdamW(model2.parameters(), lr=n_learning_rate, weight_decay=0.01)\n",
        "criterion = nn.SmoothL1Loss()\n",
        "scheduler = lr_sc(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DJUiriy0c4cz"
      },
      "outputs": [],
      "source": [
        "def get_r2(model, loader, mode):\n",
        "    model.eval()\n",
        "    n_samples = 0\n",
        "    n_correct = 0\n",
        "    res = []\n",
        "    tar = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss = []\n",
        "        for step, (x, target) in enumerate(loader):\n",
        "            x=x.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(x)\n",
        "            res.extend(output.detach().cpu())\n",
        "            tar.extend(target.detach().cpu())\n",
        "            loss = criterion(output, target)\n",
        "            if step % n_display_step == 0:\n",
        "                print('\\rloss: ', loss.item(), end='')\n",
        "    tar = np.array(tar)\n",
        "    res = np.array(res)\n",
        "    print([r2_score(tar[:,i],res[:,i]) for i in range(6)])\n",
        "    return [r2_score(tar[:,i],res[:,i]) for i in range(6)]\n",
        "\n",
        "# function to train the net\n",
        "def train_model(model, criterion, optimizer, epochs, train_loader, val_loader):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for step, (x, target) in enumerate(train_loader):\n",
        "            x = x.to(device)\n",
        "            target = target.to(device)\n",
        "            output = model(x)\n",
        "            print('\\r'+str(step),end='')\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            scheduler.step()\n",
        "\n",
        "            if step % n_display_step == 0:\n",
        "                print(\"\\rEpoch {:2d} Step {:4d} Loss {:.4f}\".format(epoch, step, loss.detach().item()), f'{scheduler.get_last_lr()[0]:.2e}', end='')\n",
        "        val_acc = get_r2(model, val_loader, 'test')\n",
        "        print(\"Epoch {:2d} Loss {:.4f} Accuracy (Train | Test) {:.4f} {:.4f}\".format(epoch, loss.detach().item(), val_acc))\n",
        "\n",
        "\n",
        "train_model(model2, criterion, optimizer, n_epochs, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sa9F6t1SeQ_t"
      },
      "outputs": [],
      "source": [
        "model2.eval()\n",
        "test_output=[]\n",
        "with torch.no_grad():\n",
        "    for step, (x, id) in enumerate(test_dataset):\n",
        "        x = x.unsqueeze(0).to(device)\n",
        "        output = model2(x).detach().cpu()\n",
        "\n",
        "        output = SCALER.inverse_transform(output).squeeze(0)\n",
        "\n",
        "        output = 10**output\n",
        "        test_output.append(np.append(np.array(id),output))\n",
        "\n",
        "df = pd.DataFrame(test_output, columns=['id']+output_col_test)\n",
        "df['id'] = df['id'].astype(int)\n",
        "df.to_csv('submissionm2.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
